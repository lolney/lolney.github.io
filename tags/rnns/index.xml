<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RNNs on Luke Olney</title>
    <link>https://lukeolney.me/tags/rnns/</link>
    <description>Recent content in RNNs on Luke Olney</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Apr 2018 02:01:00 -0700</lastBuildDate>
    
	<atom:link href="https://lukeolney.me/tags/rnns/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>World Models</title>
      <link>https://lukeolney.me/readings/world-models/</link>
      <pubDate>Wed, 04 Apr 2018 02:01:00 -0700</pubDate>
      
      <guid>https://lukeolney.me/readings/world-models/</guid>
      <description>This is an interesting &amp;ndash; paraphrasing from the article &amp;ndash; &amp;ldquo;distillation of reinforcement learning research from the past two decades,&amp;rdquo; approaching from a cognitive science perspective. It considers the paradigm of training a large (many parameter) model to build a representation of the environment and its future, then training a smaller controller model, which outputs actions, from that representation. The future model, they argue, is similar to the way that humans make decisions &amp;ndash; by considering how their actions will affect the future.</description>
    </item>
    
  </channel>
</rss>