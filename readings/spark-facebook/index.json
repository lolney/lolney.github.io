{
    "data" :  {
    "title": "Spark at scale at Facebook",
    "content": "<p>This blog post details Facebook&rsquo;s migration from Hive to Spark for a particular distributed data processing workload.</p>

<p>Spark is a distributed compute engine, allowing you to write SQL or imperative code that runs in a distributed environment with relatively few accommodations.</p>

<p>Example pipeline (Hive):</p>

<pre><code>Filter -&gt;
Group -&gt;
Reduce
</code></pre>

<p>In this writeup, the core task was to aggregate feature values for each entity_id, target_id pair and shard by entity id:</p>

<pre><code>entity_id, target_id, feature_id, feature_value

-&gt;

// Aggregation could be something like this - not explicit in writeup
entity_id, target_id: &lt;average, sum, ..&gt; feature_value for feature_id
</code></pre>

<p>In Hive, each step outputs a temporary table (stored in the HDFS) that serves as the input to the next step. Because this involved serializing and writing files to HDFS repeatedly along the way, this had a pretty severe performance cost in time and CPU hours.</p>

<p>In the Spark implementation described here, each step is instead combined into a single MapReduce job.</p>

<p>They also note some contributions to Spark to improve its reliability, where the great size of their workload revealed some cracks - especially where hardcoded limits proved to be too restrictive.</p>

<p>E.g., <a href="https://github.com/apache/spark/pull/12285/files">this pull request</a> solves a problem when a reducer&rsquo;s memory is fully allocated. Memory is taken up both by the pointer array (which is the array that&rsquo;s sorted) and the records pointed to by it. Initially, the code would reset the pointer array first (allocating new memory) then free the records, which could cause OoM errors unnecessarily.</p>
"
} 
}

