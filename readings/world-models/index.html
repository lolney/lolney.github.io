<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0"> <meta name="generator" content="Hugo 0.37.1" /> 
<title>World Models - Luke Olney</title>
<meta property="og:title" content="World Models - Luke Olney">       

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116576963-1"></script>
<script>
	window.dataLayer = window.dataLayer || [];
	function gtag() { dataLayer.push(arguments); }
	gtag('js', new Date());

	gtag('config', 'UA-116576963-1');
</script>


<link rel="stylesheet" href="https://lukeolney.me/css/main.css" media="all">
<link rel="stylesheet" href="https://lukeolney.me/css/fonts.css">
  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="https://lukeolney.me/" class="nav-logo">
    <img src="https://lukeolney.me/images/logo.png" 
         width="50" 
         height="50" 
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/projects/">Projects</a></li>
    
    <li><a href="/readings/">Readings</a></li>
    
    <li><a href="/tags/">Tags</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">
  <article class="article">
    
    <h1 class="article-title"><a href=https://github.com/worldmodels/worldmodels.github.io/blob/master/draft.md>World Models</a></h1>
    

    
    <span class="article-date">2018-04-04</span>
    



    <div class="article-content">
      <p>This is an interesting &ndash; paraphrasing from the article &ndash; &ldquo;distillation of reinforcement learning research from the past two decades,&rdquo; approaching from a cognitive science perspective. It considers the paradigm of training a large (many parameter) model to build a representation of the environment and its future, then training a smaller controller model, which outputs actions, from that representation. The future model, they argue, is similar to the way that humans make decisions &ndash; by considering how their actions will affect the future.</p>

<p>The RNN part of the model uses an approach similar to the one that <a href="https://magenta.tensorflow.org/sketch-rnn-demo">SketchRNN</a> does to predict future actions. It sends compressed version of the environment through a feed-forward RNN, taking, at each stage t, the hidden vector h<em>{t-1}, the action a</em>{t-1}, and an additional input z_{t-1}. z_t is a vector sampled from the mixture of Gaussian distributions output by a <a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/">Mixture Density Network</a>.</p>

<p>These actions are generated by the final layer of the model, the controller, which takes h_t and z_t as input.
<img src="https://raw.githubusercontent.com/worldmodels/worldmodels.github.io/master/assets/world_model_schematic.svg?sanitize=true" alt="Everything together" /></p>

<p>They conduct a variety of experiments using various combinations of the three layers, showing, eg, that the full model is better than just using the RNN as input to the controller.</p>

<p>What&rsquo;s interesting is that they can use the VAE to decode the latent vectors in the MDN-RNN model, giving a representation of the model&rsquo;s prediction of the future environment. After training the model only to learn this virtual representation, they can then train the M and C layers on it and apply it to the real environment.</p>

    </div>

    <ul class="article-taxonomy">
     
</ul>
  </article>

</main>

<footer class="footer">
  <ul class="footer-links">
    <li>
      <a href="https://github.com/lolney">
        <i class="fa fa-github"></i> Github</a>
    </li>
  </ul>
</footer>

</div>

</body>

</html>