<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0"> <meta name="generator" content="Hugo 0.37" /> 
<title>Readings - Luke Olney</title>
<meta property="og:title" content="Readings - Luke Olney">       

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>



<script type="text/javascript" src="https://lukeolney.me/js/readmore.js"></script> 


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116576963-1"></script>
<script>
	window.dataLayer = window.dataLayer || [];
	function gtag() { dataLayer.push(arguments); }
	gtag('js', new Date());

	gtag('config', 'UA-116576963-1');
</script>


<link rel="stylesheet" href="https://lukeolney.me/css/main.css" media="all">
<link rel="stylesheet" href="https://lukeolney.me/css/fonts.css">
  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="https://lukeolney.me/" class="nav-logo">
    <img src="https://lukeolney.me/images/logo.png" 
         width="50" 
         height="50" 
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/projects/">Projects</a></li>
    
    <li><a href="/readings/">Readings</a></li>
    
    <li><a href="/tags/">Tags</a></li>
    
  </ul>
</nav>

      </header>


<main class="content">

    <article>
        <header>
            <h1 class="article-title">Readings</h1>
        </header>
        
        <p>Short commentaries on technical blogs or papers that I&rsquo;ve read:</p>

    </article>
    <ul>

        <div class="list">
            
            <h2 class="list-title">2019</h2>
             <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://hacks.mozilla.org/2019/03/fast-bump-allocated-virtual-doms-with-rust-and-wasm/">Dodrio: Virtual DOMs in Rust</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 14 March 2019 ">
                <a title="Permalink" href="https://lukeolney.me/readings/rust-virtual-doms/">14 March 2019</a>
            </time>
            <ul class="article-taxonomy">
     
    <li>
        <i class="fa fa-tags"></i>
        <a href="/tags/rust">Rust</a>
        <a href="/tags/browser-tech">Browser tech</a>
        <a href="/tags/javascript">Javascript</a>
        <a href="/tags/react">React</a>
    </li>
    
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                

<p>Javascript ecosystem tools like ESLint, Babel, NPM, or Webpack have traditionally been written in Javascript and run on the Node runtime. But there&rsquo;s no requirement that tools that process our JS code be written in JS themselves, and given that these tools are often computationally demanding, there have been <a href="https://www.notionjs.com/">attempts</a> to create more performant versions in languages like Rust.</p>

<p>The project described here takes that a step further, moving a process normally handled by front end JS libraries to high-performance code via WebAssembly.</p>

<p>The virtual DOM is at the core of how React, Ember, Angular, and Vue &ndash; name any recent front end view framework &ndash; do updates: a virtual representation of the DOM, mirroring the component tree, that can be diffed on updates to avoid making unnecessary updates to the actual DOM.</p>

<p>Dodrio implements the virtual DOM in Rust and does high-throughput transfers to JS to make the actual DOM updates. This raises the question, perhaps, if those updates could in the future be done directly in Wasm via the browser API, but the result is still faster than many of the libraries they benchmarked against.</p>

<h2 id="the-dodrio-api">The Dodrio API</h2>

<p>The API gives ways to manipulate virtual DOM nodes (including state, like counters) and render them. It also includes a Wasm interface for using Javacsript components in Rust:</p>

<pre><code>#[wasm_bindgen]
extern &quot;C&quot; {
    // Import the JS `Greeting` class.
    #[wasm_bindgen(extends = Object)]
    type Greeting;

    // And the `Greeting` class's constructor.
    #[wasm_bindgen(constructor)]
    fn new(who: &amp;str) -&gt; Greeting;
}

// Construct a JS rendering component from a `Greeting` instance.
let js = JsRender::new(Greeting::new(&quot;World&quot;));

</code></pre>

<h2 id="bump-allocation">Bump allocation</h2>

<p>Bump allocators treat allocation like adding to a stack: add new objects to the end, and you can&rsquo;t deallocate existing objects (except all of them at once).</p>

<p>Dodrio bump allocates its virtual DOM trees and the DOM mutations required after each render, batching the changes that will be made to the physical DOM.</p>

<p>Diagram provided by the blog post. Two trees - the two most recent renders - must be kept in memory at a given time:</p>

<pre><code>        ------------------- Time -------------------&gt;
Tree 0: [ render | ------ | diff ]
Tree 1:          [ render | diff | ------ | diff ]
Tree 2:                          [ render | diff | ------ | diff ]
Tree 3:                                          [ render | diff | ------ | diff ]
...

</code></pre>

<p>This is a simple garbage collector that avoid any of the overhead of a general tracing/generational GC.</p>

<h2 id="diffing-and-updating-the-dom">Diffing and updating the DOM</h2>

<p>The diffing process is done by walking the two trees in unison and noting whenever children, attributes, or listeners vary between the two. These changes are translated to instructions in a stack machine, which is read and interpreted in Javascript.</p>

            </div>
        </div>
    </section>
</article>  <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://code.fb.com/core-data/apache-spark-scale-a-60-tb-production-use-case/">Spark at scale at Facebook</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 16 February 2019 ">
                <a title="Permalink" href="https://lukeolney.me/readings/spark-facebook/">16 February 2019</a>
            </time>
            <ul class="article-taxonomy">
     
    <li>
        <i class="fa fa-tags"></i>
        <a href="/tags/spark">Spark</a>
        <a href="/tags/distributed-systems">Distributed systems</a>
        <a href="/tags/case-studies">Case studies</a>
    </li>
    
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                <p>This blog post details Facebook&rsquo;s migration from Hive to Spark for a particular distributed data processing workload.</p>

<p>Spark is a distributed compute engine, allowing you to write SQL or imperative code that runs in a distributed environment with relatively few accommodations.</p>

<p>Example pipeline (Hive):</p>

<pre><code>Filter -&gt;
Group -&gt;
Reduce
</code></pre>

<p>In this writeup, the core task was to aggregate feature values for each entity_id, target_id pair and shard by entity id:</p>

<pre><code>entity_id, target_id, feature_id, feature_value

-&gt;

// Aggregation could be something like this - not explicit in writeup
entity_id, target_id: &lt;average, sum, ..&gt; feature_value for feature_id
</code></pre>

<p>In Hive, each step outputs a temporary table (stored in the HDFS) that serves as the input to the next step. Because this involved serializing and writing files to HDFS repeatedly along the way, this had a pretty severe performance cost in time and CPU hours.</p>

<p>In the Spark implementation described here, each step is instead combined into a single MapReduce job.</p>

<p>They also note some contributions to Spark to improve its reliability, where the great size of their workload revealed some cracks - especially where hardcoded limits proved to be too restrictive.</p>

<p>E.g., <a href="https://github.com/apache/spark/pull/12285/files">this pull request</a> solves a problem when a reducer&rsquo;s memory is fully allocated. Memory is taken up both by the pointer array (which is the array that&rsquo;s sorted) and the records pointed to by it. Initially, the code would reset the pointer array first (allocating new memory) then free the records, which could cause OoM errors unnecessarily.</p>

            </div>
        </div>
    </section>
</article>  
            <h2 class="list-title">2018</h2>
             <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://daniel.haxx.se/http3-explained/">HTTP/3 Explained</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 30 December 2018 ">
                <a title="Permalink" href="https://lukeolney.me/readings/http3/">30 December 2018</a>
            </time>
            <ul class="article-taxonomy">
     
    <li>
        <i class="fa fa-tags"></i>
        <a href="/tags/networking">Networking</a>
    </li>
    
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                

<p>HTTP/3 is the evolution of the HTTP standard with UDP. Itâ€™s based on a new transport-layer protocol called QUIC, which allows better parallelism of the features of HTTP/2.</p>

<p>HTTP/2 introduced multiplexing, which lets the browser send multiple requests without first waiting for the response. This appears parallel at the HTTP level, but it still relies on a single TCP connection. That means that any lost packet will hold up the chain of requests until that packet can be re-transmitted.</p>

<p>HTTP/3&rsquo;s solution continues to use a single connection, but allows transmission from other logical streams while waiting for a lost packet to be retransmitted. That requires implementing a new protocol that retains TCPâ€™s features but allows more control over transmission of packets.</p>

<h2 id="streams">Streams</h2>

<p>QUIC brings HTTP streams to the transport layer. They are independently flow-controlled and independently head-blocked, with prioritization still done at the HTTP layer.</p>

<h2 id="ossification">Ossification</h2>

<p>Changing standards like TCP comes with costs: The Internet relies on a lot of intermediaries to survive, but many are not frequently updated â€” they will often drop packets on seeing TCP options that they donâ€™t recognize. QUIC gets around this by operating purely with secure connections, preventing intermediaries from inspecting traffic.</p>

<p>This has been a problem with TCP Fast Open, the standard that allows sending data on the first handshake transmission. QUIC offers its own fast open handshake to address that.</p>

<p>Regardless, there are still barriers to using UDP as a transport - many routers block UDP traffic on the standard HTTP ports, for example. This means that HTTP/1 or /2 will still have to exist as a fallback and that connections to unknown hosts will have to be initiated that way.</p>

<h2 id="adoption">Adoption</h2>

<p>Itâ€™s already in use across much of the Internet - 7% of all Internet traffic in 2017. While the original Google-introduced protocol was just for HTTP, the IETF version includes a layer for non-HTTP protocols. The implementation for that is still in progress.</p>

<p>Adoption requires coordinating support among those who determine the path of the web, including browser vendors, Apache and nGinx, and TSL implementors.</p>

            </div>
        </div>
    </section>
</article>  <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf?icmpid=link_from_whitepapers_page">Service Discovery</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 02 December 2018 ">
                <a title="Permalink" href="https://lukeolney.me/readings/service-discovery/">02 December 2018</a>
            </time>
            <ul class="article-taxonomy">
     
    <li>
        <i class="fa fa-tags"></i>
        <a href="/tags/aws">AWS</a>
        <a href="/tags/microservices">microservices</a>
    </li>
    
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                

<p>Service discovery deals with the problem of mapping services to IPs: If a load balancer wants to send a request to an API server, how does it figure out which ip address to sent it to? This AWS whitepaper gives a high level overview and trade-offs to a couple of approaches:</p>

<h2 id="domains-names-and-load-balancers">Domains Names and Load Balancers</h2>

<p>Alias service domain names (using CNAME records) to the load balancer, which routes to the correct service based on path or host name. This is vulnerable to DNS caching, but you can mitigate that with a short TTL.</p>

<h2 id="dns">DNS</h2>

<p>You can also use DNS to route requests directly to the service. Route 53 allows you to restrict access to services in your own virtual private cloud.</p>

<h2 id="configuration-management">Configuration management</h2>

<p>Map service names to IP addresses in your configuration, which you can set on server start. In particular, you can have new service register with a central configuration manager like AWS OpsWorks.</p>

<p>This is also the basis for frameworks like <a href="https://github.com/Netflix/eureka">Netflix Eurkea</a> or <a href="https://stripe.com/blog/service-discovery-at-stripe">Consul</a>.</p>

            </div>
        </div>
    </section>
</article>  <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://openid.net/specs/openid-connect-basic-1_0.html">OpenID Connect 1.0 Spec</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 22 October 2018 ">
                <a title="Permalink" href="https://lukeolney.me/readings/openid-connect-1.0/">22 October 2018</a>
            </time>
            <ul class="article-taxonomy">
     
    <li>
        <i class="fa fa-tags"></i>
        <a href="/tags/web">Web</a>
        <a href="/tags/authentication">Authentication</a>
    </li>
    
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                <p>OpenID connect is based on OAuth 2.0, building on the simple access tokens of the latter to standardize a more complete login system. While OAuth is only specified as an authorization process, leaving authentication, if they chose to include that, up to implementors, OpenID specifies ways for the OAuth server to to answer claims about the end user. That gives it a standard way to perform both authentication and authorization.</p>

<p>It makes use in several cases of JSON Web Tokens, the format for making cryptographically-verifiable claims.</p>

<p>The main addition is that of the ID Token, which contains a &lsquo;subject identitifier&rsquo; field as well as claims about, for instance, the identity of the issuer. This is suplemented by the UserInfo request, which allows the client to ask questions about the user. Standard claims include personally-identifiable information like name and birthdate.</p>

<p>The specification provided this diagram, and I&rsquo;ve filled in the terms:</p>

<pre><code>+--------+                                   +--------+
|        |                                   |        |
|        |---(1) Authentication Request-----&gt;|        |
|        |                                   |        |
|        |  +--------+                       |        |
|        |  |        |                       |        |
|        |  |  End-  |&lt;--(2) AuthN &amp; AuthZ--&gt;|        |
|        |  |  User  |                       |        |
|  OAuth |  |        |                       | OAuth  |
| Client |  +--------+                       | Server |
|        |                                   |        |
|        |&lt;---(3) Authentication Response----|        |
|        |                                   |        |
|        |---------(4) UserInfo Request-----&gt;|        |
|        |                                   |        |
|        |&lt;--------(5) UserInfo Response-----|        |
|        |                                   |        |
+--------+                                   +--------+
</code></pre>

            </div>
        </div>
    </section>
</article>  <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://krebsonsecurity.com/2017/01/who-is-anna-senpai-the-mirai-worm-author/">Who is Anna-Senpai, the Mirai Worm Author</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 18 October 2018 ">
                <a title="Permalink" href="https://lukeolney.me/readings/mirai/">18 October 2018</a>
            </time>
            <ul class="article-taxonomy">
     
    <li>
        <i class="fa fa-tags"></i>
        <a href="/tags/security">Security</a>
    </li>
    
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                <p>This is Brian Krebs&rsquo; fascinating investigative report into the Mirai botnet, the network used to launch some of the largest DDoS attacks on the Internet to date. Krebs&rsquo; account include testimony and private conversations with the hackers themselves, who were later charged by the FBI for their activities. It reveals how much the Internet remains a free-for-all, kill-or-be-killed world, where rogue security companies can surreptitiously hijack competitors&rsquo; IP addresses<a href="https://en.wikipedia.org/wiki/BGP_hijacking">1</a> or take down their services with DDoS hits.</p>

<p>Mirai was the covert arm of a legitimate business, ProTraf Solutions, which offered DDoS protection services in the competitive Minecraft server cottage industry. In early attacks, it sought competitive advantage by DDoSing a competing service, ProxyPipe, which forced many of ProxyPipe&rsquo;s customers to switch to ProTraf.</p>

<p>Krebs is able to connect ProTraf CEO&rsquo;s description of his programming language skills to that of his blackhat persona &ndash; and professed Mirai author &ndash; Anna-Senpai. He also mentions a Github username, shared among a number of online accounts that includes a number of Minecraft forums and a Reddit account. This has several interesting posts, including awfully suspicious speculation about the nature of a DDoS attack on the Rutgers campus.</p>

<p>After the ProxyPipe CEO, remarkably, managed through several layers of ISPs to get the Ukrainian-based control servers for the botnet disconnected, Anna-Senpai taunts him with the knowledge he is responsible. This is just one piece of evidence for his petty-vindictive persona &ndash; he is constantly taking down people he gets into disagreements with, like a hapless server operator and, on multiple occasions, Krebs himself.</p>

<p>The ProxyPipe CEO evenutally makes the connection that Krebs describes between Anna-Senpai and his real life persona. In fact, the two had previously been friends and collaborators on Minecraft projects, before, he says, Anna-Senpai fell to the bad influence of places like Hackforums.net.</p>

<p>His sloppiness with his online personas and his hubris eventually does the Mirai creator in, as the FBI track him down after a broad investigation. He is indicted, and now, as of the time of writing, serves the FBI as part of a plea agreement.<a href="http://www2.philly.com/philly/business/mirai-cyberattack-rutgers-student-paras-jha-fbi-20180919.html">2</a></p>

            </div>
        </div>
    </section>
</article>  <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://googleprojectzero.blogspot.com/2018/09/a-cache-invalidation-bug-in-linux.html">Linux Cache Invalidation Bug</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 02 October 2018 ">
                <a title="Permalink" href="https://lukeolney.me/readings/linux-cache-invalidation-bug/">02 October 2018</a>
            </time>
            <ul class="article-taxonomy">
     
    <li>
        <i class="fa fa-tags"></i>
        <a href="/tags/systems">Systems</a>
        <a href="/tags/security">Security</a>
    </li>
    
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                <p>This is a detailed writeup from Project Zero about a bug in the Linux Kernel, introducing by an optimization with an unconsidered edge case.</p>

<p>A quick summary of the relevant OS concepts: Virtual Memory Areas (VMAs), which include the code segment, data segment, and heap, have special rules[1] that must be applied when a page fault occurs. The mappings of addresses to VMAs are stored in <code>vm_area_struct</code>s, which are stored in a red-black tree or possibly cached in a per-thread 4-member array. Whenever a VMA is freed in any thread for the process, all of these per-thread caches must be invalidated.</p>

<p>To avoid having to do this expensive operation, each cache is tagged with a <code>seqnum</code> equal to the <code>seqnum</code> of the per-process <code>mm_struct</code>; when the <code>seqnum</code>s are out of sync, we know to invalidate the cache. Because the <code>seqnum</code> can overflow, though, we still need to flush all the caches every time that happens.</p>

<p>In the patch in question, someone added the further optimization of simply avoiding the flush if the process is single-threaded. They reasoned that VMA lookups would always trigger the update of the cache <code>seqnum</code> and <code>mm_struct</code> <code>seqnum</code> simultaneously, avoiding the need for the flush.</p>

<p>However, this added a (normally) rather unlikely bug in the case that, immediately after incrementing the <code>mm_struct</code> <code>seqnum</code> to wrap to zero, a second thread is created, pushing the <code>seqnum</code> back up to <code>0xffffffff</code> and making the cache in the first thread appear valid again.</p>

<p>The authors manage to exploit the bug first by doing a large number of VMA remappings to trigger the overflow, then creating a fake VMA using the pointers from the deallocated one &ndash; leaked to logs by a warning message &ndash; to bypass Kernel controls. The VMA contains a fault handler that is used on page faults, which can be used, for instance, to run a binary with root privileges.</p>

<p>[1] <a href="http://students.mimuw.edu.pl/SO/Linux/Kod/include/linux/mm.h.html">http://students.mimuw.edu.pl/SO/Linux/Kod/include/linux/mm.h.html</a></p>

            </div>
        </div>
    </section>
</article>  <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://pdfs.semanticscholar.org/6a39/082e14e08fcc4f853b07e4d797163afadde1.pdf">Log Structured Merge Trees</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 19 May 2018 ">
                <a title="Permalink" href="https://lukeolney.me/readings/log-structured-merge-trees/">19 May 2018</a>
            </time>
            <ul class="article-taxonomy">
     
    <li>
        <i class="fa fa-tags"></i>
        <a href="/tags/databases">Databases</a>
        <a href="/tags/data-structures">Data Structures</a>
    </li>
    
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                <p>The log-structured merge tree is used in the storage engine of many databases, including Cassandra<sup class="footnote-ref" id="fnref:1"><a href="#fn:1">1</a></sup>. It describes a data structure, often contrasted with B-trees, that&rsquo;s mainly designed as a strategy for managing writes to disk.</p>

<p>The basic approach is this: Maintain a collection of stores, increasing geometrically in size from the top down. Store records in the top level first. When level L_i fills up, merge it into L_i+1, emptying L_i.</p>

<p>Each level is itself a key-value store. Lower levels are implemented as B+ trees, filled in breadth-first order to avoid wasting space and allow for more sequential reads. Lookups proceed by checking for the key in L_0, then proceeding to the next level if the key isn&rsquo;t found.</p>

<pre><code>        [][][]               &lt;--- L_0: memory (updated in place)
      [][][][][][]           &lt;--- L_1: disk (updated with a merge)
[][][][][][][][][][][][]     &lt;--- L_2: disk (updated with a merge)
</code></pre>

<p>The log-structured approach contrasts with the update-in-place approach: while the former does sequential writes, relying on additional sequential IO to manage the structure asynchronously, the latter does random reads &ndash; relying on a single seek for each read.</p>

<p>The bLSM<sup class="footnote-ref" id="fnref:2"><a href="#fn:2">2</a></sup> authors note the choice that web services face when storing small objects: optimize for low-latency reads, or for high-throughput writes. B-trees meet the former requirement, and LSMTs meet the latter by making sure that all writes to disk are batched.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1"><a href="https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_manage_ondisk_c.html">https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_manage_ondisk_c.html</a>
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
<li id="fn:2"><a href="http://www.eecs.harvard.edu/~margo/cs165/papers/gp-lsm.pdf">http://www.eecs.harvard.edu/~margo/cs165/papers/gp-lsm.pdf</a>
 <a class="footnote-return" href="#fnref:2"><sup>[return]</sup></a></li>
</ol>
</div>

            </div>
        </div>
    </section>
</article>  <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://blog.twitter.com/engineering/en_us/a/2010/introducing-flockdb.html">Graph Databases at Twitter</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 10 May 2018 ">
                <a title="Permalink" href="https://lukeolney.me/readings/graph-dbs-at-twitter/">10 May 2018</a>
            </time>
            <ul class="article-taxonomy">
     
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                <p>&ldquo;Graph databases&rdquo; seem to be mostly wrappers over relational databases that are optimized for certain graph operations. One example is Twitter&rsquo;s FlockDB &ndash; an apparently long-abandoned project that&rsquo;s nonetheless a solid demonstration of the principle. FlockDB uses MySQL as as storage engine, storing its edges as MySQL records.</p>

<p>Flock is optimized for queries that involve a user&rsquo;s followers and people who follow that user. Like, which of my followers is following President Obama? These are the people A who satisfy the relations follows(Me, A) and follows(A, Obama). Flock contains a single table for for all A s.t. follows(Me, A). And the reverse is also stored, so follows(A, Obama) is just as efficient. So that query ends up being a single query on the <code>me</code> partition combined with a query on the <code>Obama</code> partition.</p>

            </div>
        </div>
    </section>
</article>  <article class="preview">
    <header>
        <h1 class="post-title">
            <a href="https://github.com/bodil/im-rs/blob/master/src/hashmap.rs">An Immutable Hashmap in Rust</a>
        </h1>
        <div class="post-meta">
            <time datetime=" 09 May 2018 ">
                <a title="Permalink" href="https://lukeolney.me/readings/rust-immutable-hashmap/">09 May 2018</a>
            </time>
            <ul class="article-taxonomy">
     
    <li>
        <i class="fa fa-tags"></i>
        <a href="/tags/data-structures">Data Structures</a>
        <a href="/tags/rust">Rust</a>
        <a href="/tags/code-reading">Code Reading</a>
    </li>
    
</ul>
        </div>
    </header>
    <section class="post-excerpt">
        <div class="center">
            <i class="fa fa-spinner fa-spin"></i>
        </div>
        <div class="article-content">
            <div class="readmore" hidden=true>
                <p>This is an implementation of the &ldquo;hash array mapped trie,&rdquo; a purely functional hashmap. In an ordinary trie hashmap, the keys are stored as bitstrings in a trie, as follows:</p>

<pre><code>   0
  / \
  1 0
/ \ / \
1 0 1 0 
| | | |
X F D E
</code></pre>

<p>This saves space compared to a traditional Hashmap, and it means resizings are never necessary.</p>

<p>The HAMT improves on this by instead storing in each node a table with N (32 with 32-bit words) entries, with each entry containing a pointer to another node. When traversing or inserting a key, you consider the most significant T=log_2(N)=5 bits of the hash; this indexes to either a submap, a value, or nil. It starts out as a value, then a submap is added when a new hash collides with an existing one. This submap then behaves like the root map, but with the next 5 bits of the hash as a key. With T=2:</p>

<pre><code>00 01 10 11
|   |  |  |
|   G     O
|
00 01 10 11
|  |  |  |
E     D
</code></pre>

<p>This means that lookup time is reduced to log_N(n) from log_2(n), without taking up much additional space.</p>

<p>Another note: to save space in the tables, only the keys that point to actual values or nodes are represented. This is done by adding an N-bit bitmap to each node, with the ith bit representing whether that pointer is nil. Then, the index into the table is found by calculating the hamming weight (number of 1s) in the first i bits of the bitmap, where i is also the T-bit key into the subtable.</p>

<pre><code>// Modified above example
bitmap: 1101
0 1 2
| | |
| G O
|
...
</code></pre>

<p>This means that the table must be resized every time a new key is inserted. Still, the immutable version is significantly slower than the equivalent mutable version, by a factor of about 10 with 1000 inserts.</p>

<pre><code>test hashmap_insert_10         ... bench:       5,991 ns/iter (+/- 185)
test hashmap_insert_100        ... bench:     197,136 ns/iter (+/- 9,000)
test hashmap_insert_1000       ... bench:   2,990,670 ns/iter (+/- 133,638)
test hashmap_insert_mut_10     ... bench:       1,961 ns/iter (+/- 191)
test hashmap_insert_mut_100    ... bench:      29,460 ns/iter (+/- 2,971)
test hashmap_insert_mut_1000   ... bench:     305,106 ns/iter (+/- 17,927)
</code></pre>

            </div>
        </div>
    </section>
</article>  
        </div>

        


<ul class="page-links">
  <li 
    class="page-links-disabled">
    <a href="" aria-label="Previous"><span aria-hidden="true">&lt;&#160;</span></a>
  </li>
  
  
  
  
  
   
    
    
  
  
    <li
      class="page-links-active"><a href="/readings/">1</a>
    </li>
    
  
  
  
  
   
    
    
  
  
    <li
      ><a href="/readings/page/2/">2</a>
    </li>
    
  
  
  
  
   
    
    
  
  
    <li
      ><a href="/readings/page/3/">3</a>
    </li>
    
  
  <li
    >
    <a href="/readings/page/2/" aria-label="Next"><span aria-hidden="true">&#160;&gt;</span></a>
  </li>
</ul>



</main>

<footer class="footer">
  <ul class="footer-links">
    <li>
      <a href="https://github.com/lolney">
        <i class="fa fa-github"></i> Github</a>
    </li>
  </ul>
</footer>

</div>

</body>

</html>